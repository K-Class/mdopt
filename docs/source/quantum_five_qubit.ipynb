{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding The 5-qubit Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we decode the Surface code which protects a single qubit from all types of errors by using ``mdopt``. Here, we demonstrate direct-error input decoding, which means that the decoder takes a Pauli error as input and outputs the most likely logical operator. This pipeline is sufficient for threshold computation. In reality, the decoder could be shown a syndrome measurement, from which possible error patterns would be sampled. After each run, the algorithm yields a probability distribution over the Pauli operators (I, X, Z, Y) to apply to the encoded logical qubit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qecstruct as qc\n",
    "import qecsim.paulitools as pt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import FuncFormatter, FormatStrFormatter\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import sem\n",
    "\n",
    "from mdopt.mps.utils import marginalise, create_custom_product_state\n",
    "from mdopt.contractor.contractor import mps_mpo_contract\n",
    "from mdopt.optimiser.utils import (\n",
    "    SWAP,\n",
    "    COPY_LEFT,\n",
    "    XOR_BULK,\n",
    "    XOR_LEFT,\n",
    "    XOR_RIGHT,\n",
    ")\n",
    "\n",
    "from examples.decoding.decoding import (\n",
    "    custom_code_checks,\n",
    "    custom_code_logicals,\n",
    "    custom_code_logicals_sites,\n",
    "    custom_code_constraint_sites,\n",
    "    apply_constraints,\n",
    "    apply_bitflip_bias,\n",
    "    apply_depolarising_bias,\n",
    "    pauli_to_mps,\n",
    "    decode_css,\n",
    "    css_code_stabilisers,\n",
    "    multiply_pauli_strings,\n",
    "    generate_pauli_error_string,\n",
    ")\n",
    "from examples.decoding.visualisation import plot_parity_check_mpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilizer_generators = [\n",
    "    \"XZZXI\", # 1001011000 -> 0, 3, 5, 6\n",
    "    \"IXZZX\", # 0010010110 -> 2, 5, 6, 7\n",
    "    \"XIXZZ\", # 1000100101 -> 0, 4, 7, 8\n",
    "    \"ZXIXZ\", # 0110001001 -> 1, 2, 6, 8\n",
    "]\n",
    "\n",
    "logical_operators = [\n",
    "    \"XXXXX\", # X -> 0101010101\n",
    "    \"ZZZZZ\", # Z -> 1010101010\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 6, 8], [4, 6, 8, 10], [2, 6, 8, 10], [2, 4, 8, 10]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_code_checks(stabilizer_generators, logical_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_code_constraint_sites(stabilizer_generators, logical_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_code_logicals(stabilizer_generators, logical_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_code_logicals_sites(stabilizer_generators, logical_operators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quantum error correcting code is defined on $2 * L * (L-1) + 1 = 13$ (where $L$ is the lattice size and an extra qubit handles the boundary conditions) physical qubits and has $2$ logical operators because it encodes $1$ logical qubit. This means we will need $13*2 + 2 = 28$ sites in our MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_logicals = code.num_x_logicals() + code.num_z_logicals()\n",
    "num_sites = 2 * len(code) + num_logicals\n",
    "\n",
    "assert num_sites == 28\n",
    "assert num_logicals == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the initial state. First of all we will check that no error implies no correction. This means starting from the all-zero state followed by decoding will return the all-zero state for the logical operators (the final logical operator will thus be identity operator). Thus, we start from the all-zero state for the error and the $|+\\rangle$ state for the logicals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_state = \"0\" * (num_sites - num_logicals)\n",
    "logicals_state = \"+\" * num_logicals\n",
    "state_string = logicals_state + error_state\n",
    "error_mps = create_custom_product_state(string=state_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get the sites where the checks will be applied. We will need to construct MPOs using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_x, checks_z = css_code_checks(code)\n",
    "print(\"X checks:\")\n",
    "for check in checks_x:\n",
    "    print(check)\n",
    "print(\"Z checks:\")\n",
    "for check in checks_z:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lists display the sites where we will apply the XOR constraints. However, the MPOs will also consist of other tensors, such as SWAPs (a.k.a. the tensors' legs crossings) and boundary XOR constraints. In what follows, we define the list of these auxiliary tensors and the corresponding sites where they reside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_tensors = [XOR_LEFT, XOR_BULK, SWAP, XOR_RIGHT]\n",
    "logicals_tensors = [COPY_LEFT, XOR_BULK, SWAP, XOR_RIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_sites = css_code_constraint_sites(code)\n",
    "print(\"Full X-check lists of sites:\")\n",
    "for string in constraints_sites[0]:\n",
    "    print(string)\n",
    "print(\"Full Z-check lists of sites:\")\n",
    "for string in constraints_sites[1]:\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now again take a look at the logical operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code.x_logicals_binary())\n",
    "print(code.z_logicals_binary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to again translate them to our MPO language by changing the indices since we add the logical sites at the beginning of the MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(css_code_logicals(code)[0])\n",
    "print(css_code_logicals(code)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now goes the same operation of adding sites where auxiliary tensors should be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logicals_sites = css_code_logicals_sites(code)\n",
    "print(css_code_logicals_sites(code)[0])\n",
    "print(css_code_logicals_sites(code)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun part, MPS-MPO contraction. But first, we apply the bias channel to our error state. This is done to bias our output towards the received input. This is done by distributing the amplitude around the initial basis product state to other basis product states in the descending order by Hamming distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "renormalise = True\n",
    "result_to_explicit = False\n",
    "sites_to_bias = list(range(num_logicals, num_sites))\n",
    "error_mps = apply_bitflip_bias(\n",
    "    mps=error_mps,\n",
    "    prob_bias_list=0.05,\n",
    "    sites_to_bias=sites_to_bias,\n",
    "    renormalise=renormalise,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies, bond_dims = [], []\n",
    "\n",
    "# for the X and the Z logicals\n",
    "for i in [0, 1]:\n",
    "    error_mps, entrps, bnd_dims = apply_constraints(\n",
    "        error_mps,\n",
    "        logicals_sites[i],\n",
    "        logicals_tensors,\n",
    "        renormalise=renormalise,\n",
    "        result_to_explicit=result_to_explicit,\n",
    "        strategy=\"Optimised\",\n",
    "        return_entropies_and_bond_dims=True,\n",
    "    )\n",
    "    entropies += entrps\n",
    "    bond_dims += bnd_dims\n",
    "\n",
    "# for the X and the Z checks\n",
    "for i in [0, 1]:\n",
    "    error_mps, entrps, bnd_dims = apply_constraints(\n",
    "        error_mps,\n",
    "        constraints_sites[i],\n",
    "        constraints_tensors,\n",
    "        renormalise=renormalise,\n",
    "        result_to_explicit=result_to_explicit,\n",
    "        strategy=\"Optimised\",\n",
    "        return_entropies_and_bond_dims=True,\n",
    "    )\n",
    "    entropies += entrps\n",
    "    bond_dims += bnd_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now take a look at how the bond dimensions and entropies behave throughout the decoding process while applying the parity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(entropies, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Entropy\")\n",
    "plt.xlabel(\"Site number\")\n",
    "plt.ylabel(\"Time ←\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(bond_dims, cmap=\"viridis\", norm=LogNorm(vmin=2**1, vmax=2**10))\n",
    "cbar = plt.colorbar(\n",
    "    label=\"Bond dimension\",\n",
    "    format=FuncFormatter(lambda x, pos: f\"$2^{{{int(np.log2(x))}}}$\"),\n",
    "    ticks=[2**i for i in range(1, 11, 3)],\n",
    ")\n",
    "plt.ylabel(\"Time ←\")\n",
    "plt.xlabel(\"Site number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at how the truncation error behaves for different bond dimension cutoffs. First, let's look at the bond dimensions appearing in the MPS if we do not impose any truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_mps.bond_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_dims = [np.inf, 1024, 512, 256, 128, 64, 32, 16, 8, 4, 2]\n",
    "inv_bond_dims = [1 / bd for bd in bond_dims]\n",
    "errors = []\n",
    "for chi in tqdm(bond_dims):\n",
    "    errors.append(\n",
    "        np.linalg.norm(\n",
    "            error_mps.compress(\n",
    "                chi_max=chi, renormalise=True, return_truncation_errors=True\n",
    "            )[1]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(inv_bond_dims, errors, marker=\"o\", label=\"Truncation Error\")\n",
    "plt.xlabel(\"Inverse Max Bond Dimension\")\n",
    "plt.ylabel(\"Truncation Error\")\n",
    "plt.grid(True)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we marginalise over the message bits to get the probability distribution over the four possibilities of a logical operator: $I$, $X$, $Z$, $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_to_marginalise = list(range(num_logicals, len(error_state) + num_logicals))\n",
    "logical = marginalise(mps=error_mps, sites_to_marginalise=sites_to_marginalise).dense(\n",
    "    flatten=True, renormalise=True, norm=1\n",
    ")\n",
    "print(logical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the record, we're hunting for the most likely logical operator to be the identity operator. So that's it, we see the biggest probability assigned to the identity operator. Let's see how the probabilities of the four operators change as we change the bond dimension cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"Optimised\"\n",
    "logical_values = [[] for _ in range(4)]\n",
    "\n",
    "for max_bond_dim in tqdm(bond_dims):\n",
    "    error_state = \"0\" * (num_sites - num_logicals)\n",
    "    logicals_state = \"+\" * num_logicals\n",
    "    state_string = logicals_state + error_state\n",
    "    error_mps = create_custom_product_state(string=state_string)\n",
    "\n",
    "    error_mps = apply_depolarising_bias(\n",
    "        mps=error_mps,\n",
    "        prob_bias_list=0.1,\n",
    "        sites_to_bias=sites_to_bias,\n",
    "        renormalise=renormalise,\n",
    "    )\n",
    "    for i in [0, 1]:\n",
    "        error_mps = apply_constraints(\n",
    "            error_mps,\n",
    "            logicals_sites[i],\n",
    "            logicals_tensors,\n",
    "            renormalise=renormalise,\n",
    "            result_to_explicit=result_to_explicit,\n",
    "            strategy=strategy,\n",
    "            chi_max=max_bond_dim,\n",
    "            silent=True,\n",
    "        )\n",
    "    for i in [0, 1]:\n",
    "        error_mps = apply_constraints(\n",
    "            error_mps,\n",
    "            constraints_sites[i],\n",
    "            constraints_tensors,\n",
    "            renormalise=renormalise,\n",
    "            result_to_explicit=result_to_explicit,\n",
    "            strategy=strategy,\n",
    "            chi_max=max_bond_dim,\n",
    "            silent=True,\n",
    "        )\n",
    "\n",
    "    sites_to_marginalise = list(range(num_logicals, len(error_state) + num_logicals))\n",
    "    logical = marginalise(\n",
    "        mps=error_mps, sites_to_marginalise=sites_to_marginalise\n",
    "    ).dense(flatten=True, renormalise=True, norm=1)\n",
    "\n",
    "    for i in range(4):\n",
    "        logical_values[i].append(logical[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(inv_bond_dims, logical_values[0], marker=\"o\", label=f\"Pr(I)\")\n",
    "plt.xlabel(\"Inverse Max Bond Dimension\")\n",
    "plt.ylabel(\"Logical Value\")\n",
    "plt.title(\"Logical Values vs Bond Dimension (Optimised)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(inv_bond_dims, logical_values[1], marker=\"o\", label=f\"Pr(X)\")\n",
    "plt.plot(inv_bond_dims, logical_values[2], marker=\"o\", label=f\"Pr(Z)\")\n",
    "plt.plot(inv_bond_dims, logical_values[3], marker=\"o\", label=f\"Pr(Y)\")\n",
    "plt.xlabel(\"Inverse Max Bond Dimension\")\n",
    "plt.ylabel(\"Logical Value\")\n",
    "plt.title(\"Logical Values vs Bond Dimension (Optimised)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put all of this into a function. We'll need this to run the decoder over a bunch of single- and multiqubit errors. For this, we first generate all possible one-qubit errors and first 100 two-qubit errors using `qecsim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_qubit_paulis = list(pt.ipauli(n_qubits=len(code), min_weight=1, max_weight=1))\n",
    "two_qubit_paulis = list(pt.ipauli(n_qubits=len(code), min_weight=2, max_weight=2))[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_qubit_outputs = [\n",
    "    decode_css(code, error, bias_type=\"Bitflip\", renormalise=renormalise, silent=True)\n",
    "    for error in tqdm(one_qubit_paulis)\n",
    "]\n",
    "one_qubit_corrections_distribution = [output[0] for output in one_qubit_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_qubit_outputs = [\n",
    "    decode_css(code, error, bias_type=\"Bitflip\", renormalise=renormalise, silent=True)\n",
    "    for error in tqdm(two_qubit_paulis)\n",
    "]\n",
    "two_qubit_corrections_distribution = [output[0] for output in two_qubit_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_distribution_to_pauli(distribution):\n",
    "    mapping = {0: \"I\", 1: \"X\", 2: \"Z\", 3: \"Y\"}\n",
    "    result = []\n",
    "\n",
    "    for array in distribution:\n",
    "        max_index = np.argmax(array)\n",
    "        result.append(mapping[max_index])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(map_distribution_to_pauli(one_qubit_corrections_distribution))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(map_distribution_to_pauli(two_qubit_corrections_distribution))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check by hand that some of the decoder's nontrivial outputs are indeed correct. First of all, from all one-qubit errors we get the Identity operator which corresponds to the fact that the surface code of distance $d$ (equal to 3 in our case) corrects errors on up to $ \\lfloor \\frac{d-1}{2} \\rfloor $ qubits. However, some of the two-qubit errors can be corrected as well. Let's check some of them. For this, let's take a look at the first 20 errors which result in the Identity logical operator as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 20\n",
    "for i, correction in enumerate(\n",
    "    map_distribution_to_pauli(two_qubit_corrections_distribution)\n",
    "):\n",
    "    if correction == \"I\":\n",
    "        print(two_qubit_paulis[i])\n",
    "    if i > limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to track which parity check is triggered by which error, let's plot the tensor network we are building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parity_check_mpo(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to dive a bit more into what is happening inside the decoder to be able to better understand the results. For example, the first error $(X_0 Z_1)$ from the list above would trigger the first $X$ parity check (parity check index 2) as well as the second $Z$ parity check (index 9). In the current setup the stabilisers are being set to $0$, which is the result of the fact that the $\\text{XOR}$ tensors we use project out the inputs of odd (i.e., equal to $1$) parity. After applying the logical-operator MPOs and performing marginalization, the process yields a marginal distribution over codewords, each reflecting different parities of the logical operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the errors which result in the $X$ logical operator as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, correction in enumerate(\n",
    "    map_distribution_to_pauli(two_qubit_corrections_distribution)\n",
    "):\n",
    "    if correction == \"X\":\n",
    "        print(two_qubit_paulis[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous case, the first error $(Y_0 Z_1)$ from the list above would trigger the first $X$ parity check which in its turn would trigger the $\\text{XOR}$ tensor corresponding to the $X$ logical-operator MPO therefore the $X$ logical as the most likely output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at how the MPO order optimisation looks visually and test the truncation effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATTICE_SIZE = 3\n",
    "rep_code = qc.repetition_code(LATTICE_SIZE)\n",
    "surface_code = qc.hypergraph_product(rep_code, rep_code)\n",
    "\n",
    "mpo_matrix_full_unoptimised = plot_parity_check_mpo(\n",
    "    surface_code, optimise_order=False, return_matrix=True, plot_type=\"both\"\n",
    ")\n",
    "\n",
    "mpo_matrix_xpart = plot_parity_check_mpo(\n",
    "    surface_code, optimise_order=False, return_matrix=True, plot_type=\"X\"\n",
    ")\n",
    "\n",
    "mpo_matrix_zpart = plot_parity_check_mpo(\n",
    "    surface_code, optimise_order=False, return_matrix=True, plot_type=\"Z\"\n",
    ")\n",
    "\n",
    "mpo_matrix_full = plot_parity_check_mpo(\n",
    "    surface_code, optimise_order=True, return_matrix=True, plot_type=\"both\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUBITS = 2 * (LATTICE_SIZE - 1) * LATTICE_SIZE\n",
    "NUM_EXPERIMENTS = 200\n",
    "\n",
    "SEED = 123\n",
    "seed_seq = np.random.SeedSequence(SEED)\n",
    "errors = {}\n",
    "\n",
    "max_bond_dims = [128, 64, 32, 16, 8, 4]\n",
    "error_rates = np.linspace(0.05, 0.40, 11)\n",
    "failures_statistics = {}\n",
    "\n",
    "rep_code = qc.repetition_code(LATTICE_SIZE)\n",
    "surface_code = qc.hypergraph_product(rep_code, rep_code)\n",
    "\n",
    "for ERROR_RATE in error_rates:\n",
    "    errors[LATTICE_SIZE, ERROR_RATE] = []\n",
    "    for l in range(NUM_EXPERIMENTS):\n",
    "        rng = np.random.default_rng(seed_seq.spawn(1)[0])\n",
    "        random_integer = rng.integers(1, 10**8 + 1)\n",
    "        SEED = random_integer\n",
    "\n",
    "        error = generate_pauli_error_string(\n",
    "            len(surface_code),\n",
    "            ERROR_RATE,\n",
    "            seed=SEED,\n",
    "            error_model=\"Bit Flip\",\n",
    "        )\n",
    "        errors[LATTICE_SIZE, ERROR_RATE].append(error)\n",
    "\n",
    "for CHI_MAX in max_bond_dims:\n",
    "    print(f\"CHI_MAX = {CHI_MAX}\")\n",
    "    for ERROR_RATE in tqdm(error_rates):\n",
    "        failures = []\n",
    "\n",
    "        for l in range(NUM_EXPERIMENTS):\n",
    "            error = errors[LATTICE_SIZE, ERROR_RATE][l]\n",
    "            _, success = decode_css(\n",
    "                code=surface_code,\n",
    "                error=error,\n",
    "                chi_max=CHI_MAX,\n",
    "                multiply_by_stabiliser=True,\n",
    "                bias_type=\"Bit Flip\",\n",
    "                bias_prob=0.05,\n",
    "                renormalise=True,\n",
    "                silent=True,\n",
    "                contraction_strategy=\"Optimised\",\n",
    "            )\n",
    "            failures.append(1 - success)\n",
    "\n",
    "        failures_statistics[LATTICE_SIZE, CHI_MAX, ERROR_RATE] = failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_rates = {}\n",
    "error_bars = {}\n",
    "\n",
    "for CHI_MAX in max_bond_dims:\n",
    "    for ERROR_RATE in error_rates:\n",
    "        failure_rates[LATTICE_SIZE, CHI_MAX, ERROR_RATE] = np.mean(\n",
    "            failures_statistics[LATTICE_SIZE, CHI_MAX, ERROR_RATE]\n",
    "        )\n",
    "        error_bars[LATTICE_SIZE, CHI_MAX, ERROR_RATE] = sem(\n",
    "            failures_statistics[LATTICE_SIZE, CHI_MAX, ERROR_RATE]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "green_cmap = colormaps[\"viridis_r\"]\n",
    "norm = Normalize(vmin=0, vmax=len(max_bond_dims) - 1)\n",
    "\n",
    "for index, CHI_MAX in enumerate(max_bond_dims):\n",
    "    plt.errorbar(\n",
    "        error_rates,\n",
    "        [\n",
    "            failure_rates[LATTICE_SIZE, CHI_MAX, ERROR_RATE]\n",
    "            for ERROR_RATE in error_rates\n",
    "        ],\n",
    "        yerr=[\n",
    "            error_bars[LATTICE_SIZE, CHI_MAX, ERROR_RATE] for ERROR_RATE in error_rates\n",
    "        ],\n",
    "        fmt=\"o--\",\n",
    "        label=f\"Lattice size: {LATTICE_SIZE}, max bond dim: {CHI_MAX}\",\n",
    "        linewidth=3,\n",
    "        color=green_cmap(norm(index)),\n",
    "    )\n",
    "\n",
    "plt.legend(fontsize=7)\n",
    "plt.xlabel(\"Error rate\")\n",
    "plt.ylabel(\"Failure rate\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so, we see the convergence in bond dimension (given bitflip noise, we converege to optimal decoding at the bond dimension equal to $2^6$ where $6$ is the maximum number of leg crossings encountered while applying MPOs, thus the curves with bond dimensions $2^6$ and $2^7$ are identically the same). Besides, we see how the curve moves to the right as we increase the bond dimension cutoff which is expected behaviour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdopt-ZdbamFdU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
